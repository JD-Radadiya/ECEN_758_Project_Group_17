{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "# model_clip, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clip, preprocess = clip.load(\"ViT-B/32\", download_root=\"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # normalize the dataset to range [-1, 1]\n",
    "    transforms.RandomRotation(10),  # rotate the image randomly by 10 degrees\n",
    "    transforms.RandomHorizontalFlip()  # flip the image horizontally with a 50% probability\n",
    "])\n",
    "\n",
    "transform_clip = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))  # normalize the dataset to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create indices for training and validation splits\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Define split size\n",
    "split = int(0.2 * num_train)  # 20% for validation\n",
    "train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Create samplers for training and validation\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "# Create data loaders with samplers\n",
    "trainloader = DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "valloader = DataLoader(trainset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "testset_clip = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_clip)\n",
    "testloader_clip = torch.utils.data.DataLoader(testset_clip, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if our data is loaded correctly\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "print(images.shape)\n",
    "print(len(trainloader))\n",
    "print(labels.shape)\n",
    "print(len(testloader))\n",
    "\n",
    "# Check each sample in the dataset\n",
    "missing_samples = any(img is None or lbl is None for img, lbl in trainset)\n",
    "\n",
    "if missing_samples:\n",
    "    print(\"Missing data detected in individual samples.\")\n",
    "else:\n",
    "    print(\"No missing data found in individual samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, counts = np.unique(trainset.targets, return_counts=True)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Counts: {counts}\")\n",
    "\n",
    "print(\"Number of samples per digit:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Digit {label}: {count}\")\n",
    "\n",
    "# Check if the data distribution is balanced\n",
    "total_samples = len(trainset.targets)\n",
    "min_samples = min(counts)\n",
    "max_samples = max(counts)\n",
    "balance_ratio = min_samples / max_samples\n",
    "\n",
    "print(f\"\\nTotal samples: {total_samples}\")\n",
    "print(f\"Balance ratio (min/max): {balance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot all the unique labels from the dataset\n",
    "# Define the text labels\n",
    "text_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(text_labels[labels[idx].item()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNImage(nn.Module):\n",
    "    def __init__(self, dropout=0.45, kernel_size=3, num_classes=10):\n",
    "        super(SimpleCNNImage, self).__init__()\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization added\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization added\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)  # Batch normalization added\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)  # Batch normalization added\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Output Layer\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout1(self.relu3(self.bn3(self.fc1(x))))\n",
    "        x = self.dropout2(self.relu4(self.bn4(self.fc2(x))))\n",
    "        x1 = x\n",
    "        x = self.fc3(x)\n",
    "        return x1, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "model = SimpleCNNImage()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/best_model_state.pth\"\n",
    "best_model = SimpleCNNImage(dropout=0.45)\n",
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs=10):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            _, outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "        train_losses.append(running_train_loss / len(train_loader))\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                _, outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "                \n",
    "        valid_losses.append(running_valid_loss / len(valid_loader))\n",
    "        valid_accuracy = 100 * correct_valid / total_valid\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.2f}%, Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to tune\n",
    "    dropout = trial.suggest_float('dropout', 0.3, 0.6)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 5, 20)\n",
    "\n",
    "    # Initialize the model, criterion, and optimizer with suggested parameters\n",
    "    model = SimpleCNNImage(dropout=dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(\n",
    "        model, criterion, optimizer, trainloader, valloader, num_epochs\n",
    "    )\n",
    "\n",
    "    # Return the best validation accuracy (maximize)\n",
    "    return max(valid_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    best_model.load_state_dict(torch.load(model_path))\n",
    "    best_model.eval()\n",
    "except:\n",
    "    # Create a study and optimize\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=7)\n",
    "\n",
    "    # Print best parameters\n",
    "    print(\"Best parameters found:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    # Evaluate on the testing set with the best model\n",
    "    best_params = study.best_params\n",
    "    best_model = SimpleCNNImage(dropout=best_params['dropout'])\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "    train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(\n",
    "        best_model, criterion, optimizer, trainloader, valloader, best_params['num_epochs']\n",
    "    )\n",
    "\n",
    "    # Save the model state dict\n",
    "    model_path = \"./model/best_model_state.pth\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "    print(f\"Model state dict saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix, f1 score, and classification report\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "\n",
    "    if loader == valloader:\n",
    "        print(\"Evaluating on the validation set\")\n",
    "        flag = \"val\"\n",
    "    elif loader == testloader:\n",
    "        print(\"Evaluating on the test set\")\n",
    "        flag = \"test\"\n",
    "        test_loss = 0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "    best_model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    misclassified_images = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Testing\"):\n",
    "            _, outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "\n",
    "            # Find misclassified images\n",
    "            misclassified_mask = predicted != labels\n",
    "            misclassified_images.extend(images[misclassified_mask])\n",
    "            true_labels.extend(labels[misclassified_mask])\n",
    "            predicted_labels.extend(predicted[misclassified_mask])\n",
    "\n",
    "            if flag == \"test\":\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    if flag == \"test\":\n",
    "        test_accuracy = 100 * correct_test / total_test\n",
    "        print(f\"Test Loss: {test_loss / len(testloader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return misclassified_images, true_labels, predicted_labels, y_true, y_pred\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "misclassified_images_val, true_labels_val, predicted_labels_val, y_true_val, y_pred_val = evaluate_model(best_model, valloader)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "misclassified_images_test, true_labels_test, predicted_labels_test, y_true_test, y_pred_test = evaluate_model(best_model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds\n",
    "k_folds = 5\n",
    "\n",
    "# Set up KFold\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for each fold\n",
    "fold_train_losses = []\n",
    "fold_valid_losses = []\n",
    "fold_train_accuracies = []\n",
    "fold_valid_accuracies = []\n",
    "\n",
    "# Extract best parameters of the model\n",
    "dropout = best_params['dropout']\n",
    "learning_rate = best_params['learning_rate']\n",
    "num_epochs = best_params['num_epochs']\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(trainset)):\n",
    "    print(f'Fold {fold + 1}/{k_folds}')\n",
    "    \n",
    "    # Create samplers for training and validation\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    # Create data loaders with samplers\n",
    "    trainloader = DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "    valloader = DataLoader(trainset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = SimpleCNNImage(dropout=dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model for this fold\n",
    "    train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(\n",
    "        model, criterion, optimizer, trainloader, valloader, num_epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    # Store the results\n",
    "    fold_train_losses.append(train_losses)\n",
    "    fold_valid_losses.append(valid_losses)\n",
    "    fold_train_accuracies.append(train_accuracies)\n",
    "    fold_valid_accuracies.append(valid_accuracies)\n",
    "\n",
    "# Calculate the average performance across all folds\n",
    "avg_train_losses = np.mean(fold_train_losses, axis=0)\n",
    "avg_valid_losses = np.mean(fold_valid_losses, axis=0)\n",
    "avg_train_accuracies = np.mean(fold_train_accuracies, axis=0)\n",
    "avg_valid_accuracies = np.mean(fold_valid_accuracies, axis=0)\n",
    "\n",
    "# Plotting average training and validation metrics across all folds\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(avg_train_losses, label='Training Loss')\n",
    "plt.plot(avg_valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Average Training and Validation Loss (Across Folds)')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(avg_train_accuracies, label='Training Accuracy')\n",
    "plt.plot(avg_valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Average Training and Validation Accuracy (Across Folds)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix, f1 score, and classification report\n",
    "def plot_results(y_true, y_pred, name):\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=text_labels, yticklabels=text_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix' + f\" ({name})\")\n",
    "    plt.show()\n",
    "\n",
    "    # F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1:.4f}\" + f\" ({name})\")\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=text_labels)\n",
    "    print(\"Classification Report:\" + f\" ({name})\")\n",
    "    print(report)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\" + f\" ({name})\")\n",
    "    print(\"\\nMost Common Misclassifications\" + f\" ({name})\")\n",
    "    conf_df = pd.DataFrame(cm, index=text_labels, columns=text_labels)\n",
    "    misclassified_pairs = conf_df.stack().reset_index()\n",
    "    misclassified_pairs.columns = [\"True Label\", \"Predicted Label\", \"Count\"]\n",
    "    misclassified_pairs = misclassified_pairs[misclassified_pairs[\"True Label\"] != misclassified_pairs[\"Predicted Label\"]]\n",
    "    misclassified_pairs = misclassified_pairs.sort_values(\"Count\", ascending=False)\n",
    "    misclassified_pairs = misclassified_pairs.reset_index(drop=True)\n",
    "\n",
    "    print(misclassified_pairs.head(10))\n",
    "    print()    \n",
    "    print()\n",
    "\n",
    "# Plot the results for the validation set\n",
    "plot_results(y_true_val, y_pred_val, \"Validation\")\n",
    "\n",
    "# Plot the results for the test set\n",
    "plot_results(y_true_test, y_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize some of the misclassified images\n",
    "def visualize_misclassified_images(misclassified_images, true_labels, predicted_labels, class_names, num_images=5, name=\"\"):\n",
    "    print(f\"Misclassified images ({name})\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(num_images, len(misclassified_images))):\n",
    "        image = misclassified_images[i].cpu().numpy().squeeze()\n",
    "        true_label = class_names[true_labels[i].cpu().item()]\n",
    "        predicted_label = class_names[predicted_labels[i].cpu().item()]\n",
    "        \n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'True: {true_label}\\nPred: {predicted_label}')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Define class names for FashionMNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Visualize some of the misclassified images\n",
    "visualize_misclassified_images(misclassified_images_val, true_labels_val, predicted_labels_val, class_names, num_images=5, name=\"Validation\")\n",
    "visualize_misclassified_images(misclassified_images_test, true_labels_test, predicted_labels_val, class_names, num_images=5, name=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(testloader_clip, desc=\"Zer-Shot Classification with CLIP\"):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "        text_input = clip.tokenize([\"a photo of a \" + class_names[label] for label in labels]).to(device)\n",
    "        text_features = model.encode_text(text_input)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "        preds = similarity.argmax(dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(all_labels, all_preds, \"Zero-Shot Classification with CLIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement text to image matching from a database of FashionMNIST images\n",
    "\n",
    "def text_to_image_in_memory(text_query, loader, preprocess):\n",
    "    # Encode the text input\n",
    "    text_input = clip.tokenize(text_query).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model_clip.encode_text(text_input)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    best_match = None\n",
    "    best_similarity = -float('inf')\n",
    "\n",
    "    for idx, (images,_) in tqdm(loader, desc=\"Text to Image Matching\"):\n",
    "        with torch.no_grad():\n",
    "            image_features = model_clip.encode_image(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "            if similarity.max() > best_similarity:\n",
    "                best_similarity = similarity.max()\n",
    "                best_match = idx\n",
    "    \n",
    "    return best_match, best_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text query\n",
    "text_query = \"a photo of a sneaker\"\n",
    "best_match, best_similarity = text_to_image_in_memory(text_query, testloader_clip, preprocess)\n",
    "print(f\"Best match: {class_names[best_match]}, Similarity: {best_similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(model, image_train_loader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for image_data, image_labels in tqdm((image_train_loader), desc=f\"Extracting\"):\n",
    "            image_input = image_data.view(-1, 1, 28, 28).float()\n",
    "            image_embedding, _ = model(image_input)\n",
    "            embeddings.append(image_embedding)\n",
    "            labels.append(image_labels)\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    labels = torch.cat(labels)\n",
    "    return embeddings, labels\n",
    "\n",
    "# Extract embeddings for training data\n",
    "train_embeddings, train_labels = extract_embeddings(best_model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_embeddings)\n",
    "\n",
    "# Calculate cumulative explained variance ratio\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the index where cumulative explained variance ratio first surpasses 0.9\n",
    "index_90_percent = np.argmax(cumulative_variance >= 0.9)\n",
    "print(f\"Number of components for 90% explained variance: {index_90_percent}\")\n",
    "\n",
    "# Plot cumulative explained variance ratio\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% Explained Variance')\n",
    "plt.axvline(x=index_90_percent, color='g', linestyle='--', label='90% Variance Component')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "# output_path = os.path.join(output_dir, 'Cumulative Explained Variance Ratio Image.png')\n",
    "# plt.savefig(output_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply PCA\n",
    "def apply_pca(embeddings, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "    return pca_result\n",
    "\n",
    "# Apply PCA\n",
    "num_components = 8  # You can choose any number of components\n",
    "pca_embeddings = apply_pca(train_embeddings, num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pca_embeddings data type:\", type(pca_embeddings))\n",
    "print(\"pca_embeddings shape:\", pca_embeddings.shape)\n",
    "\n",
    "# Check the data type and shape of labels\n",
    "train_labels_np = train_labels.numpy()\n",
    "print(\"labels data type:\", type(train_labels_np))\n",
    "print(\"labels shape:\", train_labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "# Function to apply UMAP, visualize embeddings, and perform k-means clustering\n",
    "def visualize_umap_with_clustering(embeddings, labels, num_clusters):\n",
    "    # Apply UMAP for dimensionality reduction\n",
    "    umap_result = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "    \n",
    "    # Plot UMAP embeddings\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(umap_result[:, 0], umap_result[:, 1], c=labels, cmap='tab10', s=3)\n",
    "    plt.title('UMAP Visualization of Train Embeddings')\n",
    "    plt.colorbar(label='Labels')\n",
    "#     output_path = os.path.join(output_dir, 'UMAP Visualization of Train Embeddings Image.png')\n",
    "#     plt.savefig(output_path)\n",
    "    \n",
    "    # Apply k-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    cluster_labels = kmeans.fit_predict(umap_result)\n",
    "    cluster_centers = kmeans.cluster_centers_  # Get cluster centers\n",
    "    \n",
    "    # Plot UMAP embeddings with cluster assignments and cluster centers\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(umap_result[:, 0], umap_result[:, 1], c=cluster_labels, cmap='viridis', s=3)\n",
    "    plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x', s=50, label='Cluster Centers')\n",
    "    plt.title('UMAP Visualization with K-means Clustering')\n",
    "    plt.colorbar(label='Clusters')\n",
    "    plt.legend()\n",
    "#     output_path = os.path.join(output_dir, 'UMAP Visualization with K-means Clustering Image.png')\n",
    "#     plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have your embeddings stored in pca_embeddings and labels in train_labels_np\n",
    "# Also, assuming the number of clusters is 10\n",
    "visualize_umap_with_clustering(pca_embeddings, train_labels_np, num_clusters=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
